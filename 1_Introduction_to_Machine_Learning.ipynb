{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ee1674-e0a1-47b8-8a23-7a67373279cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "**In this practical session you will:**\n",
    "\n",
    "   - Learn the essential idea behind Machine Learning including several statistical concepts and the implementation steps under the point of view of the Data Science cycle.\n",
    "   - Download, explore and implement the preliminary processing of a multi-omics cancer dataset that will be used throughout the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aa72b7",
   "metadata": {},
   "source": [
    "## Definition:\n",
    "\n",
    "Machine learning is a subfield of artificial intelligence (AI) that focuses on developing algorithms and models that allow computers to learn patterns and make predictions or decisions without being explicitly programmed. In the context of data science and mathematical modeling, machine learning plays a crucial role in building models that represent real-world systems using mathematical concepts and language. A subfield of Machine learning is Deep learning, which uses a type of models called neural networks that are inspired in the architechture of human brains.\n",
    "\n",
    "[![AI diagram](http://danieljhand.com/images/AI_ML_DL_circles.jpeg)](http://danieljhand.com/the-relationship-between-artificial-intelligence-ai-machine-learning-ml-and-deep-learning-dl.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daf8763",
   "metadata": {},
   "source": [
    "## Characteristics of Machine Learning:\n",
    "\n",
    "1. **Learning from Data:**\n",
    "   - Machine learning systems learn from data rather than relying on explicit programming by using some statistical techniques.\n",
    "   - Algorithms use available data to identify patterns, relationships, and trends.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "2. **Model Development:**\n",
    "   - Machine learning involves creating models that can generalize patterns from the training data to make predictions or decisions on new, unseen data.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "3. **Adaptability:**\n",
    "   - Machine learning models can adapt and evolve as new data becomes available, making them suitable for dynamic and changing environments.\n",
    "\n",
    "<!-- Add an empty line here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d915a3e",
   "metadata": {},
   "source": [
    "## Data Science Lifecycle:\n",
    "\n",
    "The data science lifecycle involves several key steps that machine learning implementations follows:\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "1. **Identification of the problem:**\n",
    "   - Allows the decision on the suitable model and algorithm and definition of training and test datasets.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "2. **Data Collection:**\n",
    "   - Gather relevant data from various sources, ensuring it is representative and suitable for the problem at hand.\n",
    "   - It is highly important to perform exploratory analysis to evaluate the quality of the data and, if suitable, define the subsequent necessary processing steps.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "3. **Data Processing:** This is probably the most relevant step: independently of a succesful implementation of the previous steps, if the data does not contain the information relevant to solve the problem and or present in an inadequate state for the algorithm to learn from, the resulting model will be useless (garbage-in -> garbage-out). It mainly consists of two steps.\n",
    "\n",
    "    3.1. **Data Pre-processing:**\n",
    "    - Clean and preprocess the data to handle missing values, outliers, and format issues.\n",
    "\n",
    "    <!-- Add an empty line here -->\n",
    "\n",
    "    3.2. **Feature Engineering:**\n",
    "    - Necessary in some cases but optional in others.\n",
    "    - Select or create features that are relevant and informative for the machine learning model.\n",
    "    - Common approaches are grouped into *Filter-based*, *Wrapper-based* and *Embedded-based* categories.\n",
    "   \n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "4. **Data modelling:** During this iterative process, each model's performance is assessed using different metrics depending if the algorithm works with categorical or continous variables.\n",
    "\n",
    "    <!-- Add an empty line here -->\n",
    "\n",
    "    4.1. **Model Training:**\n",
    "    - Use a learning algorithm to train the model on a labeled dataset, allowing it to learn patterns and relationships.\n",
    "\n",
    "    <!-- Add an empty line here -->\n",
    "\n",
    "    4.2. **Model Optimization:**\n",
    "    - Adjust model parameters and features to improve performance, often involving techniques like hyperparameter tuning. Within this step it is important to avoid overfitting (the model could be generalized to datasets beyond the training ones).\n",
    "\n",
    "    <!-- Add an empty line here -->\n",
    "\n",
    "    4.3. **Model Testing:**\n",
    "    - Validate the model on new, unseen data to ensure it generalizes well (without overfitting) and provides accurate predictions (without underfitting).\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "5. **Deployment:**\n",
    "   - Deploy the model into a real-world environment, integrating it into decision-making processes.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Data Science LyfeCycle](https://www.onlinemanipal.com/wp-content/uploads/2022/09/Data-Science-Life-cycle-768x767.png.webp)](https://www.onlinemanipal.com/blogs/data-science-lifecycle-explained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b3293b",
   "metadata": {},
   "source": [
    "## Types of Machine Learning\n",
    "\n",
    "Machine learning is broadly categorized into several types, each serving different purposes and solving distinct problems. Here are the main types:\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![AI diagram](https://www.freecodecamp.org/news/content/images/2020/08/ml-1.png)](https://www.freecodecamp.org/news/machine-learning-for-managers-what-you-need-to-know/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    "In supervised learning, the algorithm is trained on a **labeled** dataset, where each input is paired with the corresponding output. The goal is to learn a mapping from inputs to outputs, and hence, **predict an output based on input**.\n",
    "\n",
    "The usefulness of these models is evaluated immediately since both the input and corresponding correct outputs are provided in the testing dataset.\n",
    "\n",
    "\n",
    "**a. Regression:**\n",
    "   - **Objective:** Predict a continuous target variable.\n",
    "   - **Examples:** Linear Regression, Polynomial Regression.\n",
    "\n",
    "**b. Classification:**\n",
    "   - **Objective:** Predict a discrete target variable (class labels).\n",
    "   - **Examples:** Logistic Regression, Decision Trees or Random Forest and Support Vector Machines.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "Unsupervised learning involves training on **unlabeled** data, and the algorithm tries to **discover patterns or relationships in the data** without explicit guidance on the output.\n",
    "\n",
    "Since the output is unknown in the training data, the usefulness is implicitly derived from the structure and relationships discovered in the data.\n",
    "\n",
    "**a. Clustering:**\n",
    "   - **Objective:** Group similar data points together.\n",
    "   - **Examples:** K-Means Clustering, Hierarchical Clustering.\n",
    "\n",
    "**b. Dimensionality Reduction:**\n",
    "   - **Objective:** Reduce the number of input features while preserving important information. It is also commonly used as a pre-processing step for feature extraction.\n",
    "   - **Examples:** Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE).\n",
    "\n",
    "**c. Association Rule Learning:** It will not be covered on this course.\n",
    "   - **Objective:** Discover interesting relationships between variables in large datasets.\n",
    "   - **Examples:** Apriori Algorithm, Eclat Algorithm.\n",
    "\n",
    "\n",
    "### Reinforcement Learning\n",
    "\n",
    "Reinforcement learning involves an **agent interacting with an environment**, learning to make decisions by receiving feedback in the form of rewards or penalties (mimics the human trial and error behaviour). Hence, the objective is to **learn a policy to make decisions achieving the most optimal result**.\n",
    "\n",
    "On this type of algorithms, the performance depends on the environment provided by the agent (reward or penalty) after each action, guiding it towards learning a successful policy for optimization (https://www.youtube.com/@aiwarehouse). It is usually employed for training AI for videogames rather than on -omics data analysis, so it won't be covered on this course.\n",
    "\n",
    "**a. Model-Based Reinforcement Learning:**\n",
    "\n",
    "   - **Objective:** Build an explicit model of the environment to make decisions.\n",
    "   - **Examples:** Monte Carlo Tree Search.\n",
    "\n",
    "**b. Model-Free Reinforcement Learning:**\n",
    "\n",
    "   - **Objective:** Learn to make decisions without an explicit model of the environment.\n",
    "   - **Examples:** Q-Learning, Deep Q Network (DQN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb21391",
   "metadata": {},
   "source": [
    "## Relationship with Statistical Concepts\n",
    "\n",
    "1. **Pattern Recognition:**\n",
    "   - Machine learning involves finding patterns in data, a concept deeply rooted in statistics.\n",
    "   - Depeding on the types of problem, and hence, the employed algorithm, different kinds of patterns can be extracted from data.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Pattern types](https://www.researchgate.net/profile/Gordon-Elger/publication/352727978/figure/fig2/AS:1153327744192512@1651986170131/Machine-learning-tasks-most-relevant-for-PdM.png)](https://www.researchgate.net/figure/Machine-learning-tasks-most-relevant-for-PdM_fig2_352727978)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "2. **Cross-Validation:** A key concept for supervised models when the available dataset is smaller than the optimal for the validation purposes.\n",
    "   - To assess a supervised model's generalization ability, cross-validation techniques are used to evaluate performance on multiple subsets of the data.\n",
    "   - There are multiple methodologies (https://www.turing.com/kb/different-types-of-cross-validations-in-machine-learning-and-their-explanations) although the most common is the **K-fold cross-validation** which involves partitioning the entire dataset into k number of random subsets, where k-1 are used for training and 1 for testing purposes. This is repeated for a number of iterations and the model is evaluated through the metrics obtained across interations.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Bias and Variance](https://d2mk45aasx86xg.cloudfront.net/image5_11zon_af97fe4b03.webp)](https://www.turing.com/kb/different-types-of-cross-validations-in-machine-learning-and-their-explanations) \n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "3. **Bias-Variance Tradeoff:** Also a key concept when dealing with supervised models.\n",
    "   - In statistics, the bias of an estimator is the difference between this estimator’s expected value and the true value of the parameter being estimated. On the other hand, the variance of an estimator measures how much the estimates from the estimator are likely to vary or spread out around the true, unknown parameter, through repeated sampling.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "   [![Bias and Variance](https://nvsyashwanth.github.io/machinelearningmaster/assets/images/bias_variance.jpg)](https://nvsyashwanth.github.io/machinelearningmaster/bias-variance/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "   - If we consider Machine learning predictions as estimations these two concepts acquire the following meaning in this context. \n",
    "       \n",
    "       - **Variance** is the consistency of the model predictions for a particular sample instance (for instance applying the model multiple times on subsets of the training dataset). In other words, is the sensitivity of the model to the randomness of the training dataset.\n",
    "       \n",
    "       - In contrast, **Bias** could be seen as the measure of the distance between predictions and the correct values (the labels) if we rebuild the model multiple times with different training datasets. Therefore, is the measure of the systematic error not due to randomness in the training data.\n",
    "             \n",
    "   - These two concepts are intrinsically related, and therefore, the bias-variance tradeoff is a fundamental concept in machine learning: there is an optimal model complexity that allows for good performance on the training data but still keeping the ability to generalize to new data. Deviations from these optimal area leads to either high bias (underfitting, there is still room to improve the model though training) or high variance (overfitting, excessive training on a specific dataset and unable to generalize to similar test datasets) of the model.\n",
    "   \n",
    "   <!-- Add an empty line here -->\n",
    "\n",
    "   [![Optimal complexity](https://ejenner.com/post/bias-variance-tradeoff/tradeoff_huad58a1a719791584e96223cc1385b715_74447_1200x1200_fit_q75_h2_lanczos_3.webp)](https://ejenner.com/post/bias-variance-tradeoff/)\n",
    "   \n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "   [![Underfitting and overfitting](https://www.endtoend.ai/assets/blog/misc/bias-variance-tradeoff-in-reinforcement-learning/underfit_right_overfit.png)](https://www.endtoend.ai/blog/bias-variance-tradeoff-in-reinforcement-learning/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "4. **Statistical Metrics:**\n",
    "   - Various statistical metrics are used to quantify the performance of both unsupervised, and mostly, supervised machine learning models.\n",
    "   - The type of metric used is related with the type of problem/algorithm used.\n",
    "   \n",
    "   <!-- Add an empty line here -->\n",
    "   \n",
    "   [![Supervised metrics](https://www.kdnuggets.com/wp-content/uploads/anello_machine_learning_evaluation_metrics_theory_overview_11.png)](https://www.kdnuggets.com/machine-learning-evaluation-metrics-theory-and-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bd29ed",
   "metadata": {},
   "source": [
    "## Case of use: Cancer genomics\n",
    "\n",
    "Cancer is a group of diseases involving abnormal cell growth with the potential to invade or spread to other parts of the body. At the very core of the etiology of cancer is somatic mutations: permanent alterations in the genetic material (either resulting from spontaneous errors during the DNA replication or as a result of DNA damage) originated throughout the somatic development (from the very first mitotic divisions of the Zygot to the human adult tissues).\n",
    "\n",
    "As sequencing technologies advanced in the past decade, the number of available tumoral whole genomes have increased exponentially, revealing that different tumors accumulate mutations with a variability of up to three orders of magnitude.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "![ICGC TMB](images/ICGC_muts.png)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Not only the total number of mutation varies, but also the composition. The endogenous mutational processes active in a tissue as well as the mutagens a person has been exposed during their lifetime, e.g ultraviolet (UV)-light or tobacco smoking, define a set of probabilities for each nucleotide to mutate provided of its neighboring\n",
    "sequence. These probabilities can be inferred can be decomposed from the observed data into several\n",
    "components that roughly reflect the individual mutational processes affecting the cell, the so-called ‘mutation signatures’, some linked to specific mechanisms.\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "**Tobacco-related signature of single base substitutions (SBS) 4**\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Tobacco Signature](https://cog.sanger.ac.uk/cosmic-signatures-production/images/v2_signature_profile_4.original.png)](https://cancer.sanger.ac.uk/signatures/signatures_v2/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "**Ultraviolet light-related signature of single base substitutions (SBS) 7**\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "[![Tobacco Signature](https://cog.sanger.ac.uk/cosmic-signatures-production/images/v2_signature_profile_7.original.png)](https://cancer.sanger.ac.uk/signatures/signatures_v2/)\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "<!-- Add an empty line here -->\n",
    "\n",
    "Hence, the study of mutations within the Cancer Genomics field, integrated with other -omic data such as transcriptomics or epigenomics as well as clinical data has paved the latest advances in Cancer Research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e1084",
   "metadata": {},
   "source": [
    "Several international consortium have generated multi-omic cancer datasets. One of them, enmarked within The Cancer Genome Atlas (TCGA) is the Pan Cancer Analysis of Whole Genomes (PCAWG) initiative. Public available data is stored at the International Cancer Genome Consortium (ICGC) database: https://dcc.icgc.org/releases/PCAWG\n",
    "\n",
    "Some files are particularly interesting for analysis with Machine Learning techniques:\n",
    "\n",
    "- Clinical (phenotypical) information for each donor belonging to given project, contained at **pcawg_donor_clinical_August2016_v9.xlsx** file. Here you have relevant information such as the donor sex, the vital status, the treatment, the age at diagnosis and the history of smoking and alcohol habits.\n",
    "- Relationship between donor, specimen and sample identifications at **pcawg_sample_sheet.tsv** file. A donor is the individual with cancer, where several specimens (biopsies of tumor or healthy tissue) can be collected. Moreover, from these specimens more than one samples could be collected to extract omics information (WGS, RNA-seq,...).\n",
    "- A matrix with the expression in transcript per millions (TPMs) for multiple genes across several samples. This information is contained at **pcawg.rnaseq.transcript.expr.tpm.tsv.gz**.\n",
    "<!-- - A list of known detected driver mutations on samples, contained at **TableS3_panorama_driver_mutations_ICGC_samples.public.tsv.gz**. -->\n",
    "- A matrix of the proportion of mutations attributed to a given mutation signature across specimens with Signature Analyzer. The data is contained at the **SignatureAnalyzer_COMPOSITE.SBS.txt** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b28ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can start by downloading some files, for that we will need the pandas package\n",
    "import pandas as pd\n",
    "\n",
    "# We will also need numpy for some operations\n",
    "import numpy as np\n",
    "\n",
    "# Os is a basic python integrated library. The path utilities are useful to work with local files\n",
    "from os import path\n",
    "\n",
    "# To explore the datasets it is always useful to use some plotting packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Some dependencies on the seaborn package will generate warnings due to the version. Just ignore them\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e4cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The clinical information of the donors\n",
    "## It is an excel file, so we use the function read_excel from pandas\n",
    "clinical_df = pd.read_excel('https://dcc.icgc.org/api/v1/download?fn=/PCAWG/clinical_and_histology/pcawg_donor_clinical_August2016_v9.xlsx')\n",
    "clinical_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ba2500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For starters we can explore both of these files.\n",
    "## The clinical information for donors contains mostly categorical variables but others such as the patient age at diagnosis is continous\n",
    "## Moreover, it seems there are a lot of missing data. Let's explore it.\n",
    "\n",
    "categorical_columns = ['project_code', 'donor_sex', 'donor_vital_status', 'first_therapy_type', 'first_therapy_response',\n",
    "                        'tobacco_smoking_history_indicator', 'alcohol_history', 'alcohol_history_intensity']\n",
    "\n",
    "continuous_columns = ['donor_age_at_diagnosis', 'tobacco_smoking_intensity', 'donor_survival_time', 'donor_interval_of_last_followup']\n",
    "\n",
    "# Create a list of tuples indicating whether each column is categorical or continuous\n",
    "column_types = [(col, 'categorical') if col in categorical_columns else (col, 'continuous') for col in categorical_columns + continuous_columns]\n",
    "\n",
    "n_rows = 4\n",
    "n_cols = (len(categorical_columns)+len(continuous_columns))//n_rows\n",
    "\n",
    "# Create a figure with multiple subplots (make a grid, 4 rows, 3 columns)\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(13, 16))\n",
    "\n",
    "for i, tupl in enumerate(column_types):\n",
    "    col = tupl[0]\n",
    "    cat = tupl[1]\n",
    "    if cat == 'categorical':\n",
    "        clinical_df[col].value_counts().plot.pie(autopct='%1.1f%%', ax=axes[i//n_cols, i%n_cols], startangle=90)\n",
    "        axes[i//n_cols, i%n_cols].set_title(f'Pie Chart - {col}')\n",
    "        axes[i//n_cols, i%n_cols].set_ylabel('')\n",
    "    elif cat == 'continuous':\n",
    "        sns.histplot(clinical_df[col], bins=20, kde=True, ax=axes[i//n_cols, i%n_cols])\n",
    "        axes[i//n_cols, i%n_cols].set_title(f'Histogram - {col}')\n",
    "        axes[i//n_cols, i%n_cols].set_xlabel(col)\n",
    "        axes[i//n_cols, i%n_cols].set_ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7efc345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the warnings, the code ignores on the categorical plots the Non-Available data. Let's plot it with dropna=False on the value_counts() function\n",
    "# Create a figure with multiple subplots (make a grid, 4 rows, 3 columns)\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(13, 16))\n",
    "\n",
    "for i, tupl in enumerate(column_types):\n",
    "    col = tupl[0]\n",
    "    cat = tupl[1]\n",
    "    if cat == 'categorical':\n",
    "        clinical_df[col].value_counts(dropna=False).plot.pie(autopct='%1.1f%%', ax=axes[i//n_cols, i%n_cols], startangle=90)\n",
    "        axes[i//n_cols, i%n_cols].set_title(f'Pie Chart - {col}')\n",
    "        axes[i//n_cols, i%n_cols].set_ylabel('')\n",
    "    elif cat == 'continuous':\n",
    "        sns.histplot(clinical_df[col], bins=20, kde=True, ax=axes[i//n_cols, i%n_cols])\n",
    "        axes[i//n_cols, i%n_cols].set_title(f'Histogram - {col}')\n",
    "        axes[i//n_cols, i%n_cols].set_xlabel(col)\n",
    "        axes[i//n_cols, i%n_cols].set_ylabel('Frequency')\n",
    "\n",
    "# Save the figure for future uses\n",
    "plt.savefig(path.join('plots', 'Clinical_withNA.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad6bea4",
   "metadata": {},
   "source": [
    "Notice that know except for the sex and vital status categories, the NA category nan from numpy is the most common category across the information on the patients. This is going to complicate analysis using this clinical phenotypical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a3fbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save it on the data folder for later uses.\n",
    "clinical_df.to_csv(path.join('data', 'clinical_df.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47fa1bb",
   "metadata": {},
   "source": [
    "Now we can download the file with relation of specimens and samples extracted from each donor. It is just a file that helps connect by IDs other files, so let's have a quick look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea61e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## It is a tabular separated file, so we read with the read_csv function specifying the tabulator \\t as the separator character\n",
    "## Moreover, we indicate that the file has a header that should be inferred as the column names.\n",
    "sample_df = pd.read_csv('https://dcc.icgc.org/api/v1/download?fn=/PCAWG/donors_and_biospecimens/pcawg_sample_sheet.tsv', sep='\\t', header='infer')\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5098d09",
   "metadata": {},
   "source": [
    "Note that for the same donor, several specimens are extracted. Usually, one from a normal tissue and another from a primary tumor (to find mutations through WGS it is necessary to remove germline mutations that are present on both normal and tumoral tissue, that is why the mutations found on normal tissues are usually substracted from the tumor mutation calls).\n",
    "\n",
    "Moreover, notice that from the same tumoral specimen several samples might be extracted to extract information with different techniques: in this case for WGS or for RNA-seq. This will be relevant to map multiomic information across samples from the same tumoral specimen from a given donor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8590e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge it with a file provided in the data folder to extract the primary site\n",
    "project_info = pd.read_csv(path.join('data', 'projects_PCAWG_info.txt'), sep='\\t', header='infer')\n",
    "\n",
    "primary_location_dict = dict(zip(project_info.project, project_info.primary_location))\n",
    "\n",
    "sample_df['primary_location'] = sample_df['dcc_project_code'].map(primary_location_dict)\n",
    "\n",
    "# Save it on the data folder for later uses\n",
    "sample_df.to_csv(path.join('data', 'sample_df.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416cbc4",
   "metadata": {},
   "source": [
    "Next, the expression data is downloaded. This **is a very large file and using pandas function will take time**. As a simple alternative, you can download it directly with your browser into the data folder: https://dcc.icgc.org/api/v1/download?fn=/PCAWG/transcriptome/transcript_expression/pcawg.rnaseq.transcript.expr.tpm.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d44eda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the expression data\n",
    "expression_df = pd.read_csv(path.join('data', 'pcawg.rnaseq.transcript.expr.tpm.tsv.gz'), sep='\\t', header='infer', compression='gzip')\n",
    "expression_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12ee5d6",
   "metadata": {},
   "source": [
    "Here the first column shows a the Ensembl Transcript ID and the rest of the columns, whose name is the aliquot ID (present at the **pcawg_sample_sheet.tsv** file).\n",
    "\n",
    "If we want to add gene IDs or Symbols instead of transcript IDs, we will need to process the file that the PCAWG consortium uses for annotation, located at: https://dcc.icgc.org/api/v1/download?fn=/PCAWG/drivers/expression/rnaseq.gc19_extNc.gtf.tar.gz\n",
    "\n",
    "Another alternative than using the browser is to use a bash script. In Jupyter notebooks, you can use other interpreters rather than python. The script below is able to download and process the necessary file (If you don't have a linux-based operative system, skip this step. The file is already provided in the data folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5650e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# The %% above indicates to the jupyter notebook to use bash as interpreter\n",
    "\n",
    "# Change into the data directory\n",
    "cd data\n",
    "\n",
    "# Download the file from the ICGC\n",
    "wget -O rnaseq.gc19_extNc.gtf.tar.gz https://dcc.icgc.org/api/v1/download?fn=/PCAWG/drivers/expression/rnaseq.gc19_extNc.gtf.tar.gz\n",
    "\n",
    "# Process the file using a bash code\n",
    "zcat rnaseq.gc19_extNc.gtf.tar.gz | cut -f9 | cut -d';' -f2 | sed 's/.*gencode::\\([^:]*\\)::tc_\\([^._]*\\)[^:]*::\\([^._]*\\)[^:]*.*/\\1\\t\\2\\t\\3/' | sort | uniq | tail -n +3 | gzip > gencode_transcript.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cfa031",
   "metadata": {},
   "source": [
    "Summarizing until here we have:\n",
    "\n",
    "**pcawg.rnaseq.transcript.expr.tpm.tsv.gz**: A large file with the first column being the ensembl Transcript ID and the rest of the columns with an aliquot ID.\n",
    "\n",
    "**sample_df.tsv**: A file that contains the relationship between donors, specimens and samples. The aliquotID is also a column of this file.\n",
    "\n",
    "**gencode_transcript.tsv.gz**: A file that contains the transcript information.\n",
    "\n",
    "For the analysis at the following sessions we will need to process the expression data, specifically:\n",
    "\n",
    "- Get the information on a gene, instead than on a transcript level.\n",
    "\n",
    "- Get only the expression for tumoral specimens (and that will not cover all the tumoral specimens of the PCAWG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da42517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open expression matrix\n",
    "expression_matrix = pd.read_csv(path.join('data', 'pcawg.rnaseq.transcript.expr.tpm.tsv.gz'),\n",
    "                                                                    sep=\"\\t\", header='infer', compression='gzip')\n",
    "\n",
    "expression_matrix['Transcript'] = expression_matrix['Transcript'].str.extract('^(\\w+)\\.\\w+$')\n",
    "\n",
    "expression_matrix = expression_matrix.set_index('Transcript', drop=True)\n",
    "\n",
    "\n",
    "# Specimen information PCAWG\n",
    "sample_df = pd.read_csv(path.join('data', 'sample_df.tsv'), sep=\"\\t\", header='infer')\n",
    "\n",
    "# Get an aliquot to specimen ID dictionary\n",
    "specimen_dict = dict(zip(sample_df.aliquot_id, sample_df.icgc_specimen_id))\n",
    "\n",
    "\n",
    "# Let's translate the columns into the specimen ID\n",
    "translated_columns = []\n",
    "aliq_ID_not_found_on_files = []\n",
    "for aliqID in expression_matrix.columns:\n",
    "    try:\n",
    "        translated_columns.append(specimen_dict[aliqID])\n",
    "    except:\n",
    "        aliq_ID_not_found_on_files.append(aliqID)\n",
    "        \n",
    "print('Total number of aliquots with expression data: ' + str(len(expression_matrix.columns)))\n",
    "print('Aliquot that could be translated into specimenID: ' + str(len(translated_columns)))\n",
    "print('Dropped samples because of unknown translation of IDs: ' + str(len(aliq_ID_not_found_on_files)))\n",
    "\n",
    "# Extract the columns\n",
    "print(expression_matrix.shape[1])\n",
    "expression_matrix = expression_matrix.drop(aliq_ID_not_found_on_files, axis=1)\n",
    "print(expression_matrix.shape[1])\n",
    "\n",
    "expression_matrix.columns = translated_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0082d4d0",
   "metadata": {},
   "source": [
    "Apparently all the aliquotIDs can be translated into SpecimenIDs thanks to the **sample_df.tsv**, so there was no lost of information. However, how many of these specimens that were RNA-Sequenced are from tumoral samples? We do not want on the next analysis to include non-tumoral tissues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca3a567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rom the Specimen IDs that we could obtain using the RNA-Seq, library strategy, are all specimen types from tumoral samples?\n",
    "category_series = sample_df[(sample_df['icgc_specimen_id'].isin(translated_columns))&(sample_df['library_strategy']=='RNA-Seq')]['dcc_specimen_type'].value_counts()\n",
    "category_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5fb4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a pie plot for the different categories\n",
    "category_series.plot.pie(autopct='%1.1f%%', startangle=90)\n",
    "\n",
    "# Add a title\n",
    "plt.title('Distribution of Specimen Types for RNA-Seq')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a20af",
   "metadata": {},
   "source": [
    "Most of the expression data come from specimens of primary solid tumors. Other specimens are from lymph nodes or blood (not solid primary tumors) or even metastasis. However, a non-negligible proportion comes from eaither Normal tissue adjacent to the primary tumor or just regular healthy tissues. Hence we need to remove them from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f069269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get specimens that do not come form normal healthy tissues (do not contain the normal word)\n",
    "clean_sample_df = sample_df[(sample_df['icgc_specimen_id'].isin(translated_columns))&(sample_df['library_strategy']=='RNA-Seq')&(~sample_df['dcc_specimen_type'].str.startswith('Normal'))].copy()\n",
    "# Check that no healthy tissue derived specimens remain\n",
    "print(clean_sample_df['dcc_specimen_type'].value_counts())\n",
    "\n",
    "# Remove the columns on the expression_df with expression for healthy tissue specimens\n",
    "print('Original specimens with expression: ' + str(len(expression_matrix.columns)))\n",
    "print('Specimens that belong to a tumoral tissue: ' + str(len(clean_sample_df['icgc_specimen_id'])))\n",
    "expression_matrix = expression_matrix[clean_sample_df['icgc_specimen_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c50c05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c50da",
   "metadata": {},
   "source": [
    "Finally, we need to process the matrix to get expression information at the gene level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ba797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The annotation file does not have a header, so the column names are specified\n",
    "annotation_df = pd.read_csv(path.join('data', 'gencode_transcript.tsv.gz'), \n",
    "                                sep=\"\\t\", header=None, names=['Symbol', 'Gene', 'Transcript'], compression='gzip')\n",
    "annotation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf148be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To group the transcripts and sum their expression by gene IDs we have to do the following steps\n",
    "\n",
    "## Merge the expression_matrix with annotation_df on the 'Transcript' column.\n",
    "## An Inner join is done to work with the Transcript IDs that are on both dataframes\n",
    "merged_df = pd.merge(expression_matrix.reset_index(), annotation_df , left_on='Transcript', right_on='Transcript', how='inner')\n",
    "print(\"Expression available for \" + str(len(merged_df)) + \" transcripts.\")\n",
    "\n",
    "## Group by 'Gene' and sum the values for each gene\n",
    "collapsed_df = merged_df.groupby('Gene').sum()\n",
    "\n",
    "## Drop unnecessary columns\n",
    "collapsed_df = collapsed_df.drop(columns=['Transcript', 'Symbol']).reset_index()\n",
    "\n",
    "print(\"After merging, expression for \" + str(len(collapsed_df)) + \" genes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34003fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it on the data folder for later uses.\n",
    "collapsed_df.to_csv(path.join('data' , 'gene_expression.tsv.gz'), sep='\\t', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bb131e",
   "metadata": {},
   "source": [
    "Finally, we can download the signature number of attributed mutations for each specimen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ecb0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "signatures_df = pd.read_csv('https://dcc.icgc.org/api/v1/download?fn=/PCAWG/mutational_signatures/Signatures_in_Samples/SA_Signatures_in_Samples/SA_Full_PCAWG_Attributions/SA_COMPOSITE_SNV.activity.FULL_SET.031918.txt', sep='\\t', header='infer')\n",
    "signatures_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9ca8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the tumor mutation burden to do some exploratory analysis\n",
    "TMB_proxy = signatures_df.iloc[:, 1:].sum(axis=0)\n",
    "TMB_proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the first column to extract SBS code\n",
    "signatures_df['Unnamed: 0'] = signatures_df['Unnamed: 0'].str.extract(r'_(SBS\\w+)_')\n",
    "\n",
    "# Change column names: the first is signature and the rest are the specimenID\n",
    "signatures_df.columns = ['signature'] + [col.split('__')[-1] for col in signatures_df.columns[1:]]\n",
    "\n",
    "# Save the information for later uses\n",
    "signatures_df.to_csv(path.join('data' , 'signatures.tsv.gz'), sep='\\t', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95584bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, going back to the TMB value\n",
    "specimen_IDs = [col.split('__')[-1] for col in TMB_proxy.index]\n",
    "Histological_type = [col.split('__')[0] for col in TMB_proxy.index]\n",
    "\n",
    "# Generate de novo pandas dataframe with the info\n",
    "TMB_df = pd.DataFrame({'specimenID': specimen_IDs, 'hist_type': Histological_type, 'TMB_proxy': TMB_proxy.values})\n",
    "TMB_df\n",
    "\n",
    "TMB_df.to_csv(path.join('data' , 'TMB.tsv.gz'), sep='\\t', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b244de7",
   "metadata": {},
   "source": [
    "It might be interesting to explore the data with a plot. For that we will generate a plot similar to the one showed when the case of use was introduced: a complex plot with two panels, one showing the distribution of total number of mutations for each histological class in logarithmic scale and one showing the proportion of attribution of mutations to the different signatures, across samples throughout different histological classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d03e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First set the signature name as the index (row name)\n",
    "signatures_df = signatures_df.set_index('signature')\n",
    "\n",
    "# Normalize the values in each column to generate the proportions of each signature\n",
    "signatures_df = signatures_df.div(signatures_df.sum(axis=0), axis=1)\n",
    "\n",
    "# Traspose and reorganize index to have as columns (independent variables) each signature\n",
    "signatures_df = signatures_df.transpose().reset_index()\n",
    "\n",
    "# Some signatures that were extracted at the start of the cancer genomics field were subdivided into more components\n",
    "# (7 was subdivided into 7a, 7b and 7c while 17 into 17a and 17b). To simplify we will merge into one component.\n",
    "# Create new columns by summing the specified columns\n",
    "signatures_df['SBS7a'] = signatures_df[['SBS7a', 'SBS7b', 'SBS7c']].sum(axis=1)\n",
    "signatures_df['SBS17a'] = signatures_df[['SBS17a', 'SBS17b']].sum(axis=1)\n",
    "# Rename the columns ('index' column to 'specimenID' and the others)\n",
    "signatures_df = signatures_df.rename(columns={'index': 'specimenID', \n",
    "                                              'SBS7a': 'SBS7', \n",
    "                                              'SBS17a': 'SBS17',\n",
    "                                              'SBS10a': 'SBS10'})\n",
    "# Drop the original columns\n",
    "signatures_df = signatures_df.drop(['SBS7b', 'SBS7c', 'SBS17b'], axis=1)\n",
    "\n",
    "# Drop signatures with no contribution across specimens \n",
    "sum_over = signatures_df[signatures_df.columns[1:]].sum(axis=0)\n",
    "signatures_df = signatures_df.drop(columns=list(sum_over[sum_over==0].index))\n",
    "\n",
    "# Convert TMB_proxy to logarithmic scale\n",
    "TMB_df['log_TMB_proxy'] = np.log10(TMB_df['TMB_proxy'])\n",
    "\n",
    "# Include the total number of elements in the hist_type label for the plots\n",
    "TMB_df['hist_type'] = TMB_df['hist_type'] + ' (n=' + TMB_df.groupby('hist_type').transform('count')['specimenID'].astype(str) + ')'\n",
    "\n",
    "# Merge the two dataframes\n",
    "merged_df = pd.merge(signatures_df, TMB_df , left_on='specimenID', right_on='specimenID', how='inner')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822b57ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the plotting order of hist_type by increasing median in log TMB\n",
    "order = TMB_df.groupby('hist_type')['log_TMB_proxy'].median().sort_values().index\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a figure and axes\n",
    "plt.figure(figsize=(10, 40))\n",
    "\n",
    "# Create a violin plot with a boxplot inside\n",
    "ax = sns.violinplot(y='hist_type', x='log_TMB_proxy', data=merged_df, order=order, inner='box')\n",
    "\n",
    "# Set X-axis label\n",
    "ax.set_xlabel('log(TMB_proxy)')\n",
    "\n",
    "# Set Y-axis label\n",
    "ax.set_ylabel('Hist Type')\n",
    "\n",
    "# Save the figure for future uses\n",
    "plt.savefig(path.join('plots', 'Violin.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723388e2",
   "metadata": {},
   "source": [
    "Definetly, different tumor types from the histological point of view show different levels of mutations, although there is a large variability within each histological type. For instance, it will be very difficult to distinguish by the number of mutations a sample of a bone benign tumor or a myelodisplasic syndrome type of blood cancer. But what about the composition of these mutations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3c927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Get only relevant columns\n",
    "prop_df = merged_df[merged_df.columns[:-2]].set_index('specimenID')\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=len(order), ncols=1, figsize=(5, 5 * len(order)))\n",
    "\n",
    "# Iterate over hist_type\n",
    "for i, hist_type in enumerate(order):    \n",
    "\n",
    "    # Plot the stacked bar plot on the right side\n",
    "    ax_bar = axes[i]\n",
    "    sub_prop_df = prop_df[prop_df['hist_type']==hist_type].copy()\n",
    "    sub_prop_df = sub_prop_df.drop(columns=['hist_type'])\n",
    "\n",
    "    # Step 1: Drop signatures that do not contribute to the class or less than 1% mean across samples\n",
    "    mean_over = sub_prop_df[sub_prop_df.columns].mean(axis=0)\n",
    "    sub_prop_df = sub_prop_df.drop(columns=list(mean_over[mean_over<0.01].index))\n",
    "\n",
    "    # Step 2: Identify and sort columns by contribution\n",
    "    contribution_columns = sub_prop_df.iloc[:, :-1].sum().sort_values(ascending=False).index\n",
    "\n",
    "    # Step 3: Sort rows (specimens) by total contribution of selected SBS columns\n",
    "    sorted_specimens = sub_prop_df.sort_values(by=list(contribution_columns), ascending=False)\n",
    "\n",
    "    # Automatically generate a ListedColormap with unique colors based on the number of labels\n",
    "    num_colors = len(contribution_columns)\n",
    "    color_map = plt.get_cmap('tab20', num_colors)\n",
    "\n",
    "    # Step 4: Plot the stacked bar plot\n",
    "    stacked_bar = sorted_specimens.plot(kind='bar', stacked=True, colormap=color_map, edgecolor='none', width=1, ax=ax_bar)\n",
    "    ax_bar.set_xlabel('Specimen')\n",
    "    ax_bar.set_ylabel('Contribution')\n",
    "    # Remove X-axis tick labels\n",
    "    ax_bar.set_xticklabels([])  \n",
    "    ax_bar.set_title(f'Stacked Bar Plot - {hist_type}')\n",
    "    \n",
    "    # Move the legend to the right\n",
    "    ax_bar.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Save the figure for future uses\n",
    "plt.savefig(path.join('plots', 'Barplot_signatures.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee85def",
   "metadata": {},
   "source": [
    "There is clearly larger differences in term of composition of the mutations which might help with the identification of tumor types just by using the proportions of signatures on a given sample, although there is still high variability. This might help with the decision of the type of data to use if we consider to build a model that looks on genomic data and wants to identify the histological (or even molecular subtype) of tumor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8143809e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MScOmicsVic",
   "language": "python",
   "name": "mscomicsvic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "308px",
    "width": "346px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "295px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
